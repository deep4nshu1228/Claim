# Weekly average price trend
weekly_avg = df.resample('W', on='warranty_date')['price'].mean()

plt.figure(figsize=(14, 6))
sns.regplot(
    x=np.arange(len(weekly_avg)), 
    y=weekly_avg.values,
    scatter_kws={'alpha':0.5},
    line_kws={'color':'red', 'linewidth':2}
)
plt.title('Weekly Average Price Trend with Linear Fit')
plt.xlabel('Week Index')
plt.ylabel('Average Price ($)')
plt.grid(True)

# Calculate linearity metrics
from scipy.stats import linregress
slope, intercept, r_value, p_value, std_err = linregress(
    np.arange(len(weekly_avg)), 
    weekly_avg.values
)
print(f"Linear Fit Metrics:")
print(f"R-squared: {r_value**2:.4f}")
print(f"Slope: {slope:.4f} (Price change per week)")
print(f"P-value: {p_value:.4e}")





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Sample data - replace with your actual dataframe
# Let's assume X = product_age_months, y = price
X = df[['product_age_months']].values  # Independent variable
y = df['price'].values                 # Dependent variable

# Remove missing values
mask = ~np.isnan(X) & ~np.isnan(y)
X = X[mask].reshape(-1, 1)
y = y[mask]





def get_data_by_year(df, years, date_col='warranty_taken_date', keep_year_col=False):
    # ... (same as above) ...
    return result if keep_year_col else result.drop(columns=['year'])


def get_rows_by_year(df, year): # Make sure 'year' column exists if 'year' not in df.columns: df['year'] = df['date'].dt.year # Filter rows where year matches return df[df['year'] == year]



degree = 2 coefficients = np.polyfit(x, y, degree) print("Coefficients:", coefficients)
polynomial = np.poly1d(coefficients)# Generate smooth x values for plotting the polynomial curve x_fit = np.linspace(min(x), max(x), 100) y_fit = polynomial(x_fit) plt.scatter(x, y, color='red', label='Data points') plt.plot(x_fit, y_fit, color='blue', label=f'Polynomial Fit (degree={degree})') plt.legend() plt.xlabel('x') plt.ylabel('y') plt.title('Polynomial Fit Example') plt.show()
# Predicted y values at original x points y_pred = polynomial(x) # Calculate R² manually ss_res = np.sum((y - y_pred) ** 2) ss_tot = np.sum((y - np.mean(y)) ** 2) r2 = 1 - (ss_res / ss_tot) print("R² score:", r2)




import pandas as pd import numpy as np from scipy.stats import chi2_contingency def cramers_v(cat1: pd.Series, cat2: pd.Series) -> float: tab = pd.crosstab(cat1, cat2) chi2, _, _, _ = chi2_contingency(tab, correction=False) n = tab.values.sum() phi2 = chi2 / n r, k = tab.shape # bias-corrected Cramér’s V phi2_corr = max(0, phi2 - (k-1)*(r-1)/(n-1)) r_corr = r - (r-1)**2/(n-1) k_corr = k - (k-1)**2/(n-1) return np.sqrt(phi2_corr / min((k_corr-1), (r_corr-1))) v = cramers_v(df['plant'], df['zone']) print(f"Cramér’s V = {v:.3f}")




from sklearn.preprocessing import LabelEncoder encoder = LabelEncoder() df['color_encoded'] = encoder.fit_transform(df['color'])
