import numpy as np
import matplotlib.pyplot as plt

# Check the distribution of actual values
print(f"Train actual - Min: {y_train.min()}, Max: {y_train.max()}, Mean: {y_train.mean()}")
print(f"Test actual - Min: {y_test.min()}, Max: {y_test.max()}, Mean: {y_test.mean()}")

# Plot distributions
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.hist(y_train, bins=30, alpha=0.7, label='Train')
plt.title('Training Data Distribution')
plt.subplot(1, 2, 2)
plt.hist(y_test, bins=30, alpha=0.7, label='Test')
plt.title('Test Data Distribution')
plt.show()



import pandas as pd

# Example df
# df = pd.DataFrame({"WARRANTY_COST": [100, 120, 130, 125, 140]})

# Create lag features
df['lag_1'] = df['WARRANTY_COST'].shift(1)
df['lag_2'] = df['WARRANTY_COST'].shift(2)
df['lag_3'] = df['WARRANTY_COST'].shift(3)

# Backward differences
df['backward_diff_lag1'] = df['lag_1'] - df['lag_2']
df['backward_diff_lag2'] = df['lag_2'] - df['lag_3']

# Trend slope over last 3 intervals (same as your function)
df['trend_slope'] = ((df['lag_1'] - df['lag_2']) + (df['lag_2'] - df['lag_3'])) / 2

# Time index (starting at 1)
df['time_index'] = range(1, len(df) + 1)

print(df)










# Create datetime index
df['date'] = pd.to_datetime(df[['dispatch_year', 'dispatch_month']].assign(day=1))
df = df.set_index('date').sort_index()

# Encode categorical features
from sklearn.preprocessing import LabelEncoder
label_encoders = {}
for col in ['plant', 'model']:
    le = LabelEncoder()
    df[col + '_encoded'] = le.fit_transform(df[col])
    label_encoders[col] = le

# Calculate age in months
df['age_months'] = (df['warranty_year'] - df['dispatch_year']) * 12 + \
                   (df['warranty_month'] - df['dispatch_month'])

# Add derived seasonal feature
df['dispatch_quarter'] = (df['dispatch_month'] - 1) // 3 + 1
 






train = df[df.index.year <= 2024]
test = df[df.index.year == 2025]

endog_train = train['cost']
exog_features = ['plant_encoded', 'model_encoded', 'warranty_month', 
                'warranty_year', 'age_months', 'dispatch_quarter']
exog_train = train[exog_features]

endog_test = test['cost']
exog_test = test[exog_features]





from statsmodels.tsa.statespace.sarimax import SARIMAX

model = SARIMAX(endog_train,
                exog=exog_train,
                order=(1,1,1),
                seasonal_order=(1,1,1,12))

results = model.fit(disp=False)
print(results.summary())




forecast = results.forecast(steps=len(test), exog=exog_test)

# Add forecast into dataframe
test['predicted_cost'] = forecast

# Evaluate performance
from sklearn.metrics import mean_absolute_error, mean_squared_error
mae = mean_absolute_error(endog_test, forecast)
rmse = mean_squared_error(endog_test, forecast, squared=False)

print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}")
 






from xgboost import XGBRegressor
from statsmodels.tsa.arima.model import ARIMA

# 1) Base model
feature_cols = ['lag1','lag7','ma7','ma28','dow','month','promo','...']
X_train, y_train = df[feature_cols], df['y']
base = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6)
base.fit(X_train, y_train)

# 2) Residuals
df['y_hat_base'] = base.predict(X_train)
df['resid'] = df['y'] - df['y_hat_base']

# 3) ARIMA on residuals (tune p,d,q)
arima_resid = ARIMA(df['resid'], order=(1,0,1)).fit()

# 4) Forecast
# Prepare future feature dataframe with same columns
X_future = future_df[feature_cols]
y_hat_base_future = base.predict(X_future)
resid_forecast = arima_resid.forecast(steps=len(X_future))
y_hat_hybrid = y_hat_base_future + resid_forecast.values




































train_df['base_pred'] = base_model.predict(X_train)

# -----------------------
# 3) Fit ARIMA on residuals (Univariate on Price residual only)
# -----------------------
train_df['residual'] = train_df[target_col] - train_df['base_pred']

# Order can be tuned using AIC/BIC or pmdarima.auto_arima
arima_model = ARIMA(train_df['residual'], order=(1,0,1))
arima_res   = arima_model.fit()

# -----------------------
# 4) Forecast future
# -----------------------
# Base forecasts for test set
test_df = test_df.copy()
test_df['base_pred'] = base_model.predict(X_test)

# Residual forecasts for same horizon
resid_forecast = arima_res.forecast(steps=len(test_df))

# Hybrid prediction = base + residual
test_df['hybrid_pred'] = test_df['base_pred'] + resid_forecast.values

# -----------------------
# 5) Evaluate
# -----------------------
from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_test, test_df['hybrid_pred'])
rmse = np.sqrt(mean_squared_error(y_test, test_df['hybrid_pred']))

print(f"Hybrid MAE: {mae:.2f}")
print(f"Hybrid RMSE: {rmse:.2f}")

# -----------------------
# 6) Future forecast beyond known data
# -----------------------
# Suppose you have future_df with same features but no Price yet
# - Compute base prediction
# - Forecast ARIMA residuals for that horizon
# - Sum them for final forecast

future_df = future_df.sort_index()
future_base = base_model.predict(future_df[feature_cols])
future_resid_forecast = arima_res.forecast(steps=len(future_df))
future_hybrid_forecast = future_base + future_resid_forecast.values

print("Future hybrid forecast:")
print(future_hybrid_forecast)






explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)



# Map each one-hot column to its base variable
group_map = {}
for col in X.columns:
    base_name = col.split('_')[0]  # before first underscore
    group_map[col] = base_name



# Mean absolute SHAP value per column
shap_importance = pd.DataFrame({
    'feature': X.columns,
    'shap_importance': np.abs(shap_values).mean(axis=0)
})

# Group by the original variable
grouped_importance = shap_importance.groupby(shap_importance['feature'].map(group_map))['shap_importance'].sum()

# Sort descending
grouped_importance = grouped_importance.sort_values(ascending=False)
print(grouped_importance)


import matplotlib.pyplot as plt

grouped_importance.plot(kind='bar')
plt.ylabel("Total SHAP Importance")
plt.title("Grouped SHAP Importance")
plt.show()

 
