import numpy as np
import matplotlib.pyplot as plt

# Check the distribution of actual values
print(f"Train actual - Min: {y_train.min()}, Max: {y_train.max()}, Mean: {y_train.mean()}")
print(f"Test actual - Min: {y_test.min()}, Max: {y_test.max()}, Mean: {y_test.mean()}")

# Plot distributions
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.hist(y_train, bins=30, alpha=0.7, label='Train')
plt.title('Training Data Distribution')
plt.subplot(1, 2, 2)
plt.hist(y_test, bins=30, alpha=0.7, label='Test')
plt.title('Test Data Distribution')
plt.show()



import pandas as pd

# Example df
# df = pd.DataFrame({"WARRANTY_COST": [100, 120, 130, 125, 140]})

# Create lag features
df['lag_1'] = df['WARRANTY_COST'].shift(1)
df['lag_2'] = df['WARRANTY_COST'].shift(2)
df['lag_3'] = df['WARRANTY_COST'].shift(3)

# Backward differences
df['backward_diff_lag1'] = df['lag_1'] - df['lag_2']
df['backward_diff_lag2'] = df['lag_2'] - df['lag_3']

# Trend slope over last 3 intervals (same as your function)
df['trend_slope'] = ((df['lag_1'] - df['lag_2']) + (df['lag_2'] - df['lag_3'])) / 2

# Time index (starting at 1)
df['time_index'] = range(1, len(df) + 1)

print(df)










# Create datetime index
df['date'] = pd.to_datetime(df[['dispatch_year', 'dispatch_month']].assign(day=1))
df = df.set_index('date').sort_index()

# Encode categorical features
from sklearn.preprocessing import LabelEncoder
label_encoders = {}
for col in ['plant', 'model']:
    le = LabelEncoder()
    df[col + '_encoded'] = le.fit_transform(df[col])
    label_encoders[col] = le

# Calculate age in months
df['age_months'] = (df['warranty_year'] - df['dispatch_year']) * 12 + \
                   (df['warranty_month'] - df['dispatch_month'])

# Add derived seasonal feature
df['dispatch_quarter'] = (df['dispatch_month'] - 1) // 3 + 1
 






train = df[df.index.year <= 2024]
test = df[df.index.year == 2025]

endog_train = train['cost']
exog_features = ['plant_encoded', 'model_encoded', 'warranty_month', 
                'warranty_year', 'age_months', 'dispatch_quarter']
exog_train = train[exog_features]

endog_test = test['cost']
exog_test = test[exog_features]





from statsmodels.tsa.statespace.sarimax import SARIMAX

model = SARIMAX(endog_train,
                exog=exog_train,
                order=(1,1,1),
                seasonal_order=(1,1,1,12))

results = model.fit(disp=False)
print(results.summary())




forecast = results.forecast(steps=len(test), exog=exog_test)

# Add forecast into dataframe
test['predicted_cost'] = forecast

# Evaluate performance
from sklearn.metrics import mean_absolute_error, mean_squared_error
mae = mean_absolute_error(endog_test, forecast)
rmse = mean_squared_error(endog_test, forecast, squared=False)

print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}")
 






from xgboost import XGBRegressor
from statsmodels.tsa.arima.model import ARIMA

# 1) Base model
feature_cols = ['lag1','lag7','ma7','ma28','dow','month','promo','...']
X_train, y_train = df[feature_cols], df['y']
base = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6)
base.fit(X_train, y_train)

# 2) Residuals
df['y_hat_base'] = base.predict(X_train)
df['resid'] = df['y'] - df['y_hat_base']

# 3) ARIMA on residuals (tune p,d,q)
arima_resid = ARIMA(df['resid'], order=(1,0,1)).fit()

# 4) Forecast
# Prepare future feature dataframe with same columns
X_future = future_df[feature_cols]
y_hat_base_future = base.predict(X_future)
resid_forecast = arima_resid.forecast(steps=len(X_future))
y_hat_hybrid = y_hat_base_future + resid_forecast.values

 
