from sklearn.model_selection import RepeatedKFold, cross_val_score
cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)

def cv_mae(estimator, X, y):
    mae = -cross_val_score(estimator, X, y,
                           cv=cv, scoring="neg_mean_absolute_error")
    return mae.mean(), mae.std()
print(cv_mae(model, X_train, y_train))




model = xgb.XGBRegressor(
    objective="reg:squarederror",
    n_estimators=800,          # more trees but simpler ones
    learning_rate=0.03,        # a bit smaller
    max_depth=3,               # ↓ from 6
    min_child_weight=10,       # ↑ from 1
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0.1,                 # require gain ≥0.1 to split
    tree_method="hist",
    reg_lambda=1.0,            # L2 weight
)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(
        n_estimators=300,
        max_depth=4,
        min_samples_leaf=5,
        max_features=0.7,
        random_state=42)
print("RF", cv_mae(rf, X, y))
